{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "1. Text classification\n",
    "2. Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\\\n",
    "    Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df[\"id\"], df[\"text\"], df[\"label\"]\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)[\"label\"]\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"id\", \"label\"]\n",
    "    df.to_csv(file_name, index=False)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "\n",
    "    return [ps.stem(token).lower() for token in tokens]    \n",
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
    "    :param max_size: the max size of feature dict, type: int\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n",
    "    else:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"]\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]\n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_index_vector(feats, feats_dict, max_len):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    :param feats: a list of features, type: list\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(max_len, dtype=np.int64)\n",
    "    for i, f in enumerate(feats):\n",
    "        if i == max_len:\n",
    "            break\n",
    "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, 1)\n",
    "        vector[i] = f_idx\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 3978\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/ans.csv\"\n",
    "pred_file = \"data/pred.csv\"\n",
    "min_freq = 3\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "\n",
    "# extract features\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_feats = train_stemmed\n",
    "test_feats = test_stemmed\n",
    "\n",
    "# build a mapping from features to indices\n",
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats),\n",
    "    min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN architecture\n",
    "\n",
    "In this tutorial, we will try to use the recurrent neural network for text classification.\n",
    "\n",
    "![RNN for Text](rnn_for_text.png)\n",
    "\n",
    "The RNN consists of three parts: (1) the word representation part, (2) the recurrent part, and (3) the fully connected part. The word representation part is the word embedding layer; the recurrent part includes multiple (bi-directional) recurrent layers to memorize and summarize contextualized word features; the fully connected part utilizes a multi-layer perceptron to make predictions.\n",
    "\n",
    "\n",
    "### Formula\n",
    "\n",
    "Input: $[w_1, w_2, \\cdots, w_n]$\n",
    "\n",
    "Model: \n",
    "1. Embedding layer: $[e_1, e_2, \\cdots, e_n]$\n",
    "2. RNN -> $[h_1, h_2, \\cdots, h_n]$\n",
    "3. Retrieve the last hidden state $h_n$ as the output embedding for the whole sentence.\n",
    "\n",
    "Output layer:\n",
    "1. Dense layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_rnn_layers: the number of layers of the RNN, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param rnn_type: the type of RNN, type: str\n",
    "    :param bidirectional: whether to use bidirectional rnn, type: bool\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a RNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # recurrent layers document: https://keras.io/layers/recurrent\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    \n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ####### Recurrent Layers #######\n",
    "    ################################\n",
    "    # recurrent layers\n",
    "    # Referennce: https://keras.io/api/layers/#recurrent-layers\n",
    "    if rnn_type == \"rnn\":\n",
    "        fn = SimpleRNN\n",
    "    elif rnn_type == \"lstm\":\n",
    "        fn = LSTM\n",
    "    elif rnn_type == \"gru\":\n",
    "        fn = GRU\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    h = emb\n",
    "    for i in range(num_rnn_layers):\n",
    "        is_last = (i == num_rnn_layers-1)\n",
    "        if bidirectional:\n",
    "            h = Bidirectional(fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=not is_last))(h)\n",
    "            # return_sequences:\n",
    "            # Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "            # [h_1, h_2, ..., h_n] or h_n\n",
    "        else:\n",
    "            h = fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=not is_last)(h)\n",
    "        h = Dropout(dropout_rate, seed=0)(h)\n",
    "    \n",
    "    ################################\n",
    "    #### Fully Connected Layers ####\n",
    "    ################################\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                      bias_initializer=\"zeros\",\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add residual connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\",\n",
    "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "              bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "\n",
    "# build the feats_matrix\n",
    "# convert each example to a index vector, and then stack vectors as a matrix\n",
    "train_feats_matrix = np.vstack(\n",
    "    [get_index_vector(f, feats_dict, max_len) for f in train_feats])\n",
    "test_feats_matrix = np.vstack(\n",
    "    [get_index_vector(f, feats_dict, max_len) for f in test_feats])\n",
    "\n",
    "# convert labels to label_matrix\n",
    "num_classes = max(train_labels)\n",
    "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = keras.utils.to_categorical(train_labels-1, num_classes=num_classes)\n",
    "test_label_matrix = keras.utils.to_categorical(test_labels-1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 19ms/step - loss: 0.8069 - accuracy: 0.6780\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.4762 - accuracy: 0.4575\n",
      "training loss: 0.8068780303001404 training accuracy 0.6779999732971191\n",
      "test loss: 1.4762108325958252 test accuracy 0.45750001072883606\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "hidden_size = 100 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model = build_RNN(max_len, len(feats_dict), embedding_size,\n",
    "              hidden_size, num_classes,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=0)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100, batch_size=100, verbose=0,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "An RNN Language model is provided here.\n",
    "\n",
    "Input:\n",
    "- word tokens $[w_1, w_2, \\cdots, w_n]$\n",
    "\n",
    "Model：\n",
    "- embedding layer: get the representation of all the words as $[e_1, e_2, \\cdots, e_n]$.\n",
    "- RNN: get the hidden representation of the sentence $[h_1, h_2, \\cdots, h_n]$.\n",
    "- Objective: Minimize the log probability of the sentence.\n",
    "\n",
    "\n",
    "**Chain rule:**\n",
    "\n",
    "$P(w_1w_2\\cdots w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1w_2)\\cdots$\n",
    "\n",
    "**Markov approximation**\n",
    "\n",
    "$P(w_1w_2\\cdots w_n) \\approx P(w_1|w_0)P(w_2|w_1)P(w_3|w_2)\\cdots$\n",
    "\n",
    "$P(w_1w_2\\cdots w_n) \\approx \\prod_{i=1}^{n}P(w_i|h_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "(32389, 30)\n",
      "(32389, 30, 1)\n",
      "Vocab Size 9860\n"
     ]
    }
   ],
   "source": [
    "from ptb_loader import load_data\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    \"\"\"\n",
    "    Calculate Perplexity\n",
    "    \"\"\"\n",
    "    def __init__(self, test_data, model):\n",
    "        self.test_data = test_data\n",
    "        self.model = model\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        x_probs = self.model.predict(x)\n",
    "        ppl = self.evaluate_batch_ppl(x_probs,y)\n",
    "        print('\\nValidation Set Perplexity: {0:.2f} \\n'.format(ppl))\n",
    "    @staticmethod\n",
    "    def evaluate_ppl(x, y):\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        y = y.reshape(-1)\n",
    "        return np.exp(np.mean(-np.log(np.diag(x[:, y]))))\n",
    "    def evaluate_batch_ppl(self, x, y):\n",
    "        eval_batch_size = 8\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        y = y.reshape(-1)\n",
    "        ppl = 0.0\n",
    "        for i in range(math.ceil(len(x)/eval_batch_size)):\n",
    "            batch_x = x[i*eval_batch_size:(i+1)*eval_batch_size,:]\n",
    "            batch_y = y[i*eval_batch_size:(i+1)*eval_batch_size]\n",
    "            ppl += np.sum(np.log(np.diag(batch_x[:, batch_y])))\n",
    "        return np.exp(-ppl/x.shape[0])\n",
    "\n",
    "print('Loading data')\n",
    "x_train, y_train, x_valid, y_valid, vocabulary_size, vocab = load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "num_training_data = x_train.shape[0]\n",
    "sequence_length = x_train.shape[1]\n",
    "\n",
    "print('Vocab Size',vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "drop = 0.5\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "embedding_dim = 10\n",
    "\n",
    "# lstm parameters\n",
    "hidden_size = 10\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "# inputs -> [batch_size, sequence_length]\n",
    "\n",
    "emb_layer = Embedding(input_dim=vocabulary_size, \n",
    "                    output_dim=embedding_dim, \n",
    "                    input_length=sequence_length)\n",
    "# emb_layer.trainable = False\n",
    "# if you uncomment this line, the embeddings will be untrainable\n",
    "\n",
    "embedding = emb_layer(inputs)\n",
    "# embedding -> [batch_size, sequence_length, embedding_dim]\n",
    "\n",
    "drop_embed = Dropout(drop)(embedding) \n",
    "# dropout at embedding layer\n",
    "\n",
    "# add a LSTM here, set units=hidden_size, return_sequences=True\n",
    "# Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "lstm_out_1 = LSTM(units=hidden_size, return_sequences=True)(drop_embed)\n",
    "# NER [tag1, tag2, tag3, ...]\n",
    "# output: lstm_out_1 -> [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "\n",
    "# add a TimeDistributed here, set layer = Dense(units=vocabulary_size,activation='softmax')\n",
    "# please read  https://keras.io/layers/wrappers/\n",
    "# output: outputs -> [batch_size, sequence_length, vocabulary_size]\n",
    "outputs = TimeDistributed(Dense(units=vocabulary_size,\n",
    "    activation='softmax'))(lstm_out_1)\n",
    "# [batch_size, sequence_length, output_size]\n",
    "\n",
    "# End of Model Architecture\n",
    "# ----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 10)            98600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 10)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 10)            840       \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 30, 9860)          108460    \n",
      "=================================================================\n",
      "Total params: 207,900\n",
      "Trainable params: 207,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Traning Model...\n",
      "Epoch 1/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 6.5066\n",
      "Validation Set Perplexity: 450.09 \n",
      "\n",
      "4049/4049 [==============================] - 133s 33ms/step - loss: 6.5065\n",
      "Epoch 2/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 6.0937\n",
      "Validation Set Perplexity: 365.04 \n",
      "\n",
      "4049/4049 [==============================] - 118s 29ms/step - loss: 6.0936\n",
      "Epoch 3/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.9352\n",
      "Validation Set Perplexity: 321.49 \n",
      "\n",
      "4049/4049 [==============================] - 111s 27ms/step - loss: 5.9352\n",
      "Epoch 4/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 5.8269\n",
      "Validation Set Perplexity: 291.54 \n",
      "\n",
      "4049/4049 [==============================] - 109s 27ms/step - loss: 5.8269\n",
      "Epoch 5/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.7595\n",
      "Validation Set Perplexity: 276.49 \n",
      "\n",
      "4049/4049 [==============================] - 114s 28ms/step - loss: 5.7596\n",
      "Epoch 6/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.7157\n",
      "Validation Set Perplexity: 267.84 \n",
      "\n",
      "4049/4049 [==============================] - 124s 31ms/step - loss: 5.7157\n",
      "Epoch 7/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.6834\n",
      "Validation Set Perplexity: 260.92 \n",
      "\n",
      "4049/4049 [==============================] - 118s 29ms/step - loss: 5.6835\n",
      "Epoch 8/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 5.6590\n",
      "Validation Set Perplexity: 254.94 \n",
      "\n",
      "4049/4049 [==============================] - 113s 28ms/step - loss: 5.6590\n",
      "Epoch 9/10\n",
      "4048/4049 [============================>.] - ETA: 0s - loss: 5.6364\n",
      "Validation Set Perplexity: 249.97 \n",
      "\n",
      "4049/4049 [==============================] - 113s 28ms/step - loss: 5.6364\n",
      "Epoch 10/10\n",
      "4047/4049 [============================>.] - ETA: 0s - loss: 5.6174\n",
      "Validation Set Perplexity: 245.53 \n",
      "\n",
      "4049/4049 [==============================] - 116s 29ms/step - loss: 5.6173\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = keras.optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print(\"Traning Model...\")\n",
    "history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[TestCallback((x_valid,y_valid),model=model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [vocab[s] for s in \"i visited the campus last monday\".split()]\n",
    "sent_2 = [vocab[s] for s in \"i visited the campus last pizza\".split()]\n",
    "sent_1_input = np.expand_dims(np.array(sent_1 + [0] * (x_train.shape[1]-len(sent_1))), 0)\n",
    "sent_2_input = np.expand_dims(np.array(sent_2 + [0] * (x_train.shape[1]-len(sent_2))), 0)\n",
    "sent_1_y = np.expand_dims([sent_1[1:]+[sent_1[0]]], -1)\n",
    "sent_2_y = np.expand_dims([sent_2[1:]+[sent_2[0]]], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of sentence 1: 2119.398\n",
      "perplexity of sentence 2: 3703.2559\n"
     ]
    }
   ],
   "source": [
    "print(\"perplexity of sentence 1:\", TestCallback.evaluate_ppl(model.predict(sent_1_input)[:len(sent_1)], \n",
    "                          sent_1_y))\n",
    "print(\"perplexity of sentence 2:\", TestCallback.evaluate_ppl(model.predict(sent_2_input)[:len(sent_1)], \n",
    "                          sent_2_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
