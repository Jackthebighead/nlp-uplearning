{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Naive Bayes on Text 实现基于朴素贝叶斯的文本分类**\n",
    "\n",
    "- 贝叶斯公式:\n",
    "  - $P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
    "- 在text data上应用朴素贝叶斯方法\n",
    "  - 朴素贝叶斯：假定各特征对结果的影响是相互独立的\n",
    "  - 数据\n",
    "    - 预处理\n",
    "      - 分词\n",
    "      - 去停用词\n",
    "      - 建立词典，每个满足一定阈值（出现的频率）的词会被写入词典，最后词典的key value是单词和index\n",
    "    - BOW词袋模型，用向量表示词语\n",
    "      - 建立BOW\n",
    "        - 初始化，以doc数量为纵轴，vocab的数量（词典长度）横轴，用scipy.sparse的lil_matrix建表data_matrix\n",
    "        - 对每个doc的每个word，根据dict查每个word对应vocab的index，并在横轴标记1\n",
    "        - 遍历完毕，表中每一行是一个doc，每一行为1的是该doc中出现的所有word在dict中的位置\n",
    "        - 将data_matrix转化为scipy.sparse的tocsr转换为csr matrix存储\n",
    "      - BOW矩阵的shape是|N|\\*|V|, N是doc的数量，V是vocab的数量\n",
    "    - 对标签数据做one-hot处理，最后标签矩阵的shape是|N|\\*|K|, K是标签数量\n",
    "  - 文本贝叶斯\n",
    "    - 求P(y): \n",
    "      - 先求label的frequency: np.sum(label,axis=0,keepdim=True),累加每行算出每个标签出现的频数\n",
    "      - 再求label的probability: label_freq / np.sum(label_freq,axis=0,keepdim=False)，归一化，每个标签的频数除以频数总数得到概率。\n",
    "      - 最后P(y)为(|k|,)因为keepdim为False\n",
    "    - 求P(x|y):\n",
    "      - 先求对词v出现在标签k类文本中的frequency: train_bow.transpose().dot(label)，即(K,N)\\*(N,V)\n",
    "      - 再求probability: frequency / np.sum(frequency,axis=0,keepdim=False)，除数是行归一化的frequency代表标签i出现词语的总数，所以结果矩阵中的ij代表**词i在j标签文本中出现的概率**，这是likelihood P(x|y)\n",
    "    - 但文本贝叶斯在$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$即预测阶段的x是一系列词语，即整个doc，不是单个词语或特征，所以这里应该写为：\n",
    "      - $P(y|x) = \\frac{\\sum_{i\\in doc} P(x_i|y)P(y)}{P(x)} = \\frac{P(y,x_1,x_2,...,x_n)}{P(x)}$\n",
    "      - 更细致地说，这里的doc属于Vocabulary但同一个词可能出现多次。故应该写为：\n",
    "      - $P(y|x) = \\frac{\\sum_{v\\in V} P(x_v|y)^{c(x_v)}P(y)}{P(x)}$，其中$c(x_v)$代表v单词在该doc中出现的频数，因为同一个单词可能多次是出现所以得乘方，且所有的词都可能出现所以得求和所有词语。\n",
    "        - 计算该式，使用log将变成${\\sum_{v\\in V}c(x_v)logP(x_v|y)}+logP(y)$，我们想到$c(x_v)$代表v单词在该doc中出现的频数，即BOW的定义，所以该式即BOW矩阵乘以logP(x|y)矩阵。\n",
    "      - 根据定义，这个结果就是对|N|个doc的预测类别，即结果是(|N|,|K|)\n",
    "      - 最后np.argmax(axis=1)，计算出类别即可。\n",
    "    - 简化计算\n",
    "      - 忽略p(x)\n",
    "      - 用log将乘转化为加\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to load document id, raw text and labels from the input csv file.\n",
    "The input csv file (data/train.csv or data/test.csv) has the following 3 columns:\n",
    "1. id: document id\n",
    "2. text: document raw text\n",
    "3. label: document label (data/train.csv: one of the values in {1,2,3,4,5}; data/test.csv: -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of documents, a list of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df['id'], df[\"text\"], df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to load document labels from the input csv file.\n",
    "The input csv file (data/answer.csv) has the following 2 columns:\n",
    "1. id: document id\n",
    "2. label: document label (one of the values in {1,2,3,4,5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for filtering stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for building the Bag Of Word (BOW) representations of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation of scipy lil matrix: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html\n",
    "\n",
    "Documentation of scipy csr matrix: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagofwords(data, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of tokenized documents, type: list\n",
    "    :param vocab_dict: a mapping from words to indices, type: dict\n",
    "    return a BOW matrix in Compressed Sparse Row matrix format, type: scipy.sparse.csr_matrix\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    The BOW matrix is first constructed using Row-based list of lists sparse matrix (LIL) format.\n",
    "    LIL is a convenient format for constructing sparse matrices, as it supports flexible slicing, \n",
    "    and it is efficient to change to the matrix sparsity structure.\n",
    "    '''\n",
    "    \n",
    "    data_matrix = sparse.lil_matrix((len(data), len(vocab_dict)))\n",
    "\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc:\n",
    "            word_idx = vocab_dict.get(word, -1)\n",
    "            if word_idx != -1:\n",
    "                data_matrix[i, word_idx] += 1\n",
    "                \n",
    "    '''\n",
    "    After constructing the BOW matrix on all input documents, we convert the matrix to Compressed Sparse \n",
    "    Row (CSR) format for fast arithmetic and matrix vector operations.\n",
    "    '''\n",
    "    data_matrix = data_matrix.tocsr()\n",
    "    \n",
    "    return data_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load document ids, raw texts, and labels from the train and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/answer.csv\"\n",
    "\n",
    "\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 2000\n",
      "Size of test set: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train set: {}\".format(len(train_ids)))\n",
    "print(\"Size of test set: {}\".format(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the raw texts in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [tokenize(text) for text in train_texts] \n",
    "test_tokens = [tokenize(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words from the tokenized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [filter_stopwords(tokens) for tokens in train_tokens]\n",
    "test_tokens = [filter_stopwords(tokens) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary (i.e., a mapping from words to indices) on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a set data structure to hold all words appearing in the train set\n",
    "vocab = set()\n",
    "\n",
    "for i, doc in enumerate(train_tokens):# enumerate over each document in the train set\n",
    "    # enumerate over each word in the document\n",
    "    for word in doc:\n",
    "        # if this word has been added into the set before, \n",
    "        # then it will be ignored, otherwise, it will be \n",
    "        # added into the set.\n",
    "        vocab.add(word)\n",
    "        \n",
    "# create a dictionary from the set of words, where the\n",
    "# keys are word strings and the values are numerical indices\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab:  16120\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocab: ', len(vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the BOW matrices from the tokenized texts in train and test sets respectively, using the vocabulary and the get_bagofwords function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix = get_bagofwords(train_tokens, vocab_dict)\n",
    "test_data_matrix = get_bagofwords(test_tokens, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of train_data_matrix:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Type of test_data_matrix:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of train_data_matrix: (2000, 16120)\n",
      "Shape of test_data_matrix: (400, 16120)\n"
     ]
    }
   ],
   "source": [
    "print('Type of train_data_matrix: ', type(train_data_matrix))\n",
    "print('Type of test_data_matrix: ', type(test_data_matrix))\n",
    "print('Shape of train_data_matrix:', train_data_matrix.shape)\n",
    "print('Shape of test_data_matrix:', test_data_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following symbols:\n",
    "\n",
    "N_train = size of the train set\n",
    "\n",
    "N_test = size of the test set\n",
    "\n",
    "V = vocabulary size\n",
    "\n",
    "K = number of classes\n",
    "\n",
    "All indices of tensors are 0-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train:  2000\n",
      "N_test:  400\n",
      "V:  16120\n",
      "K:  5\n"
     ]
    }
   ],
   "source": [
    "# get the size of the train set \n",
    "N_train = train_data_matrix.shape[0]\n",
    "\n",
    "# get the size of the test set \n",
    "N_test = test_data_matrix.shape[0]\n",
    "\n",
    "# get the vocabulary size\n",
    "V = len(vocab_dict)\n",
    "\n",
    "# get the number of classes\n",
    "K = max(train_labels)\n",
    "\n",
    "print('N_train: ', N_train)\n",
    "print('N_test: ', N_test)\n",
    "print('V: ', V)\n",
    "print('K: ', K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to normalize (with/without laplace smoothing) an input tensor over the first dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(P, smoothing_prior=0):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    Input: [1,2,1,2,4]\n",
    "    Output: [0.1,0.2,0.1,0.2,0.4] (without laplace smoothing) or \n",
    "    [0.1333,0.2,0.1333,0.2,0.3333] (with laplace smoothing and the smoothing prior is 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the size of the first dimension\n",
    "    N = P.shape[0]\n",
    "    \n",
    "    # sum the tensor over the first dimension\n",
    "    # setting axis = 0 means the summation is performed over the first dimension\n",
    "    # setting keepdims=True means the reduced axes (i.e., the 0-th axis this case) \n",
    "    # are left in the result as dimensions with size one. With this option, the \n",
    "    # result will broadcast correctly against the input array.\n",
    "    \n",
    "    norm = np.sum(P, axis=0, keepdims=True)\n",
    "    \n",
    "    # perform the normalization by dividing the input tensor by the norm,\n",
    "    # and add smoothing prior in both the numerator and the denominator.\n",
    "    return (P + smoothing_prior) / (norm + smoothing_prior*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to compute the accuracy score given the ground truth labels and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pre):\n",
    "    acc = accuracy_score(y_true, y_pre)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "\n",
    "1. the training labels (1-d array of shape (N_train,));\n",
    "\n",
    "2. the BOW matrix of training documents (scipy.sparse.csr_matrix of shape (N_train,V)),\n",
    "\n",
    "the training of Naive Bayes classifier is to compute the following two probabilities:\n",
    "\n",
    "1. prior: P(y) (an 1-d array with shape (K,), where the entry at position [l] is the is the prior probability of label l+1);\n",
    "\n",
    "2. likelihood:  P(x|y) (a matrix with shape (V,K), where the entry at position [i,l] is the probability of word i in the documents of label l+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix with shape (N_train,K), where the entry at\n",
    "# the position (i,j) is 1  \n",
    "# iff the (i+1)-th document belongs to (j+1)-th \n",
    "# class, otherwise it is 0\n",
    "\n",
    "data_label_onehot_matrix = np.zeros((N_train, K))\n",
    "\n",
    "for i, l in enumerate(train_labels):\n",
    "    # the (i+1)-th document has label l, so we \n",
    "    # set the entry at the position [i,l-1] to \n",
    "    # be 1\n",
    "    data_label_onehot_matrix[i, l-1] = 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_label_onehot_matrix.shape:  (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "print('data_label_onehot_matrix.shape: ', data_label_onehot_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the the labels of the first three documents in the train set and the first three rows of data_label_onehot_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels of the first three documents\n",
      "[4 2 3]\n",
      "\n",
      "The first three rows of data_label_onehot_matrix\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('The labels of the first three documents')\n",
    "print(train_labels.values[:3])\n",
    "print()\n",
    "print('The first three rows of data_label_onehot_matrix')\n",
    "print(data_label_onehot_matrix[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the frequencies of all labels in the train set by row-wise summation.\n",
    "\n",
    "Set axis = 0 so that the summation is across rows of the data_label_onehot_matrix.\n",
    "\n",
    "Set keepdims = False so that we can get an 1-d array of shape (K,) after the summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq = np.sum(data_label_onehot_matrix, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(label_freq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the frequencies of all labels in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\tFrequency\n",
      "1\t173.0\n",
      "2\t188.0\n",
      "3\t243.0\n",
      "4\t593.0\n",
      "5\t803.0\n"
     ]
    }
   ],
   "source": [
    "print('Label\\tFrequency')\n",
    "for l, f in enumerate(label_freq):\n",
    "    print('{}\\t{}'.format(l+1,f))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute P(y) by normalizing the label frequencies with laplace smoothing, where the smoothing prior = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = normalize(label_freq, smoothing_prior=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_y.shape:  (5,)\n"
     ]
    }
   ],
   "source": [
    "print('P_y.shape: ', P_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\tPrior probability\n",
      "1\t0.08678304239401496\n",
      "2\t0.0942643391521197\n",
      "3\t0.12169576059850375\n",
      "4\t0.29625935162094763\n",
      "5\t0.400997506234414\n"
     ]
    }
   ],
   "source": [
    "print('Label\\tPrior probability')\n",
    "for l, p in enumerate(P_y):\n",
    "    print('{}\\t{}'.format(l+1,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a matrix word_freq of shape (V,K), where word_freq[i,j] is the frequency of word i in the documents of label (j+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_matrix.shape:  (2000, 16120)\n",
      "train_data_matrix.transpose().shape:  (16120, 2000)\n",
      "data_label_onehot_matrix.shape:  (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "print('train_data_matrix.shape: ', train_data_matrix.shape)#(N_train,V)\n",
    "print('train_data_matrix.transpose().shape: ', train_data_matrix.transpose().shape)#(V,N_train)\n",
    "print('data_label_onehot_matrix.shape: ', data_label_onehot_matrix.shape)#(N_train,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = train_data_matrix.transpose().dot(data_label_onehot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16120, 5)\n"
     ]
    }
   ],
   "source": [
    "print(word_freq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_freq[i,j] = the dot product of the following 2 vectors:\n",
    "\n",
    "1. the i-th row of train_data_matrix.transpose(): \n",
    "\n",
    "2. the j-th column of data_label_onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i-th row of train_data_matrix.transpose() is the frequncies of word i in all documents in the train set (i.e., train_data_matrix.transpose()[i,k] is the frequency of word i in (k+1)-th document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The j-th column of data_label_onehot_matrix is a vector indicating whether each document in the train set has label (j+1) (i.e., data_label_onehot_matrix[k,j] = 1 if the (k+1)-th document has label (j+1), otherwise it is data_label_onehot_matrix[k,j] = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dot product of these two vectors is to sum over the frequencies of word i in all the train documents of label (j+1), which is the frequency of word i in the documents of label (j+1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the word_freq matrix over the rows (i.e., across all words in the vocabulary for each label) to get P(x|y) (a matrix with shape (V,K), where the entry at position [i,l] is the probability of word i in the documents of label l+1). The normalization is with laplace smoothing, where the smoothing prior = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_xy = normalize(word_freq,smoothing_prior=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_xy.shape (16120, 5)\n"
     ]
    }
   ],
   "source": [
    "print('P_xy.shape', P_xy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the probabilities of the first three word in the vocabulary in documents of every label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_xy[:3, :]: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.88433804e-05, 5.52806877e-05, 2.55369136e-05, 1.47312287e-05,\n",
       "        1.15783624e-05],\n",
       "       [2.88433804e-05, 5.52806877e-05, 2.55369136e-05, 1.47312287e-05,\n",
       "        2.31567247e-05],\n",
       "       [2.88433804e-05, 2.76403438e-05, 2.55369136e-05, 2.94624575e-05,\n",
       "        1.15783624e-05]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('P_xy[:3, :]: ')\n",
    "P_xy[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "\n",
    "1. a BOW matrix : scipy.sparse.csr_matrix of shape (N,V) (N = N_train or N = N_test);\n",
    "\n",
    "2. prior: P(y);\n",
    "\n",
    "3. likelihood:  P(x|y),\n",
    "\n",
    "the prediction of Naive Bayes classifier is to compute the following array:\n",
    "\n",
    "1. pred: an 1-d array of shape (N,), where the entry at position i is the predicted label for the i-th document in the given BOW matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the joint probability $P(Y=y_i, \\mathbf{D}) = P(Y=y_i) \\prod_{j}^{V}{P(x_j|Y=y_i)^{c(x_j)}}$, where $\\mathbf{D}$ is the data,  using the following equation:\n",
    "\n",
    "$P(Y=y_i) \\prod_{j}^{V}{P(x_j|Y=y_i)^{c(x_j)}} = \\exp(\\log(P(Y=y_i)) + \\sum_{j}^{V}{c(x_j)\\log(P(x_j|Y=y_i))})$,\n",
    "\n",
    "where $c(x_j)$ is the total count of word $x_j$ in $\\mathbf{D}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both the exponential and logarithmic function are monotonically increasing, we use them to convert the multiplication into summation in order to speed up computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute $\\log(P(Y=y_i))$, to enable the later opeartions with matrix, we use np.expand_dims function to insert a new dimension with size 1 into the first dimension of the array np.log(P_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_P_y = np.expand_dims(np.log(P_y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_P_y.shape:  (1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"log_P_y.shape: \",log_P_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute $\\log(P(x_j|Y=y_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_P_xy = np.log(P_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_P_xy.shape:  (16120, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"log_P_xy.shape: \",log_P_xy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute $\\sum_{j}^{V}{c(x_j)\\log(P(x_j|Y=y_i))}$ using the dot product between the BOW matrix and log_P_xy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_P_dy = train_data_matrix.dot(log_P_xy)\n",
    "test_log_P_dy = test_data_matrix.dot(log_P_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_log_P_dy.shape:  (2000, 5)\n",
      "test_log_P_dy.shape:  (400, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_log_P_dy.shape: \", train_log_P_dy.shape)\n",
    "print(\"test_log_P_dy.shape: \", test_log_P_dy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute $\\log(P(Y=y_i)) + \\sum_{j}^{V}{c(x_j)\\log(P(x_j|Y=y_i))}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_P = log_P_y + train_log_P_dy\n",
    "test_log_P = log_P_y + test_log_P_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_log_P.shape:  (2000, 5)\n",
      "test_log_P.shape:  (400, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_log_P.shape: \", train_log_P.shape)\n",
    "print(\"test_log_P.shape: \", test_log_P.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $P(Y=y_i |\\mathbf{D}) \\propto P(Y=y_i, \\mathbf{D})$, we directly use the log joint probabilities computed above to get labels for every document by choosing the maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add 1 because labels strat from 1\n",
    "train_pred = np.argmax(train_log_P, axis=1) + 1\n",
    "test_pred = np.argmax(test_log_P, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pred.shape:  (2000,)\n",
      "test_pred.shape:  (400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_pred.shape: \", train_pred.shape)\n",
    "print(\"test_pred.shape: \", test_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8565\n",
      "Test Accuracy: 0.5225\n"
     ]
    }
   ],
   "source": [
    "train_acc= evaluate(train_labels, train_pred)\n",
    "print(\"Train Accuracy: {}\".format(train_acc))\n",
    "\n",
    "test_acc= evaluate(test_labels, test_pred)\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
